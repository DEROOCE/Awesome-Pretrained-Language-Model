# Awesome Pretrained Language Models [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)


Pretrained Language Models (PLMs) have been achieved great success over  many NLP tasks. And with the rapid development of PLMs, there is a need to make a list to show these succesful and remarkable language models.


#  Lists of Language Models

  * [Alibaba](#alibaba)
  * [Amazon](#amazon)
  * [Baidu](#baidu)
  * [BigSicence](#bigsicence)
  * [BAAI (Beijing)](#baai-beijing)
  * [DeepMind](#deepmind)
  * [EleutherAI](#eleutherai)
  * [Facebook (Meta)](#facebook-meta)
  * [Google](#google)
  * [Huawei](#huawei)
  * [Inspur](#inspur)
  * [JD (JINGDONG)](#jd-jingdong)
  * [Langboat](#langboat)
  * [Microsoft](#microsoft)
  * [Nvidia](#nvidia)
  * [OpenAI](#openai)
  * [OPPO](#oppo)
  * [Tencent](#tencent)
  * [More PLMs are coming ...](#more-plms-are--coming-)


## Alibaba 

* PLUG 

## Amazon 

* AlexaTM 

## Baidu 

* ERNIE family
  * ERNIE 1.0
  * ERNIE 2.0
  * ERNIE Tiny
  * ERNIE GEN
  * ERNIE Gram 
  * ERNIE Doc
  * ERNIE 3.0
  * ERNIE M 
  * ERNIE 3.0 TiTan
  * ERNIE 3.0 Zeus 
* PLATO family
  * PLATO-1 
  * PLATO-2
  * PLATO-XL

## BigSicence 

* T0 series
* BLOOM 


 
## BAAI 

* Chinese-Transformer-XL 
* CPM family
  * CPM-1 
  * CPM-2 
* GLM 
* EVA family
  * EVA 1.0 
  * EVA 2.0 

 
## DeepMind 

* Gopher 
* Chinchilla
* AlphaCode 
* Retro


 
## EleutherAI

* GPT-Neo
* GPT-J
* GPT-NeoX 


 
## Facebook (Meta)

* XGLM
* MOE LM 
* XLM-R 
* M2M-100
* Blender
* OPT
* NLLB 



 
## Google 

* T5 family
  * T5 series 
  * ByT5 
  * ExT5 
* LaMDA 
* FLAN 
* GShard 
* Meena
* PaLM 
* UL2 
* Minerva 


 
## Huawei 

* PanGu (盘古) family 
  * PanGu-Alpha 
  * PanGu-Coder
* NEZHA (哪吒) 


 
## Inspur 

* Yuan 1.0 

## IDEA

* [封神榜——中文语言预训练模型开源计划](https://fengshenbang-doc.readthedocs.io/zh/latest/index.html)
  * Erlangshen (二郎神)
  * Wenzhong (闻仲)
  * Rangdeng (燃灯)
  * Yuyuan (余元)
  * Bigan (比干)
  * Zhouwenwang (周文王)
  * Taiyi (太乙)

> IDEA refers to [粤港澳大湾区数字经济研究院](https://idea.edu.cn/)

 
## JD (JINGDONG)

* 织女 Vega v1


 
## Langboat 

* Mengzi
  * paper: [Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese](https://arxiv.org/abs/2110.06696)
  * code: https://github.com/Langboat/Mengzi


 
## Microsoft 

* Megatron-Turning NLG family 
* DeBERTa series 
* Turning NLG 
* DeepNet 
* GODEL 
* Metro-LM 
* Z-code M3 


 
## Nvidia

* Megatron-LM 


 
## OpenAI

* GPT-family
  * GPT, GPT-2, GPT-3  
  * WebGPT
  * InstructGPT
* Codex 

 
## OPPO


* OBERT 






 
## Tencent 


* Motian 
* ShenZhou 
* ShenNonG
* TI-NLP
* HunYuan_nlp  


 
## THU

* THUDM
  * Wenhui (文汇)
    * code: https://github.com/THUDM/Chinese-Transformer-XL

> THUDM refers to "Data Mining Research Group at Tsinghua University".
 
* Tsinghua AI
  * CPM family
    * CPM-1
      * paper: [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413)
      * code: https://github.com/TsinghuaAI/CPM-1-Generate
    * CPM-2
      * paper: [CPM-2: Large-scale cost-effective pre-trained language models](https://www.sciencedirect.com/science/article/pii/S2666651021000310)
      * code: https://github.com/TsinghuaAI/CPM-2-Pretrain

## More PLMs are  coming ...

🔥Coming soon!🔥


--- 

# Claims 

Thanks for all these companies and orgnizations paying a lot of money and efforts to build and train these large models for the benefit of all human beings. 


> This repo is inspired by [awesome pretrained Chinese NLP models](https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models).


