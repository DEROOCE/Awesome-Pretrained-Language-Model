# Awesome Pretrained Language Models [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)


Pretrained Language Models (PLMs) have been achieved great success over  many NLP tasks. And with the rapid development of PLMs, there is a need to make a list to show these succesful and remarkable language models.


#  Lists of Language Models

  * [Alibaba](#alibaba)
  * [Amazon](#amazon)
  * [Baidu](#baidu)
  * [BigSicence](#bigsicence)
  * [BAAI (Beijing)](#baai-beijing)
  * [DeepMind](#deepmind)
  * [EleutherAI](#eleutherai)
  * [Facebook (Meta)](#facebook-meta)
  * [Google](#google)
  * [Huawei](#huawei)
  * [Inspur](#inspur)
  * [JD (JINGDONG)](#jd-jingdong)
  * [Langboat](#langboat)
  * [Microsoft](#microsoft)
  * [Nvidia](#nvidia)
  * [OpenAI](#openai)
  * [OPPO](#oppo)
  * [Tencent](#tencent)
  * [More PLMs are coming ...](#more-plms-are--coming-)


## Alibaba 

* PLUG 

## Amazon 

* AlexaTM 

## Baidu 

* ERNIE family
  * ERNIE 1.0
  * ERNIE 2.0
  * ERNIE Tiny
  * ERNIE GEN
  * ERNIE Gram 
  * ERNIE Doc
  * ERNIE 3.0
  * ERNIE M 
  * ERNIE 3.0 TiTan
  * ERNIE 3.0 Zeus 
* PLATO family
  * PLATO-1 
  * PLATO-2
  * PLATO-XL

## BigSicence 

* T0 series
* BLOOM 


 
## BAAI 

* Chinese-Transformer-XL 
* CPM family
  * CPM-1 
  * CPM-2 
* GLM 
* EVA family
  * EVA 1.0 
  * EVA 2.0 

 
## DeepMind 

* Gopher 
* Chinchilla
* AlphaCode 
* Retro


 
## EleutherAI

* GPT-Neo
* GPT-J
* GPT-NeoX 


 
## Facebook (Meta)

* XGLM
* MOE LM 
* XLM-R 
* M2M-100
* Blender
* OPT
* NLLB 



 
## Google 

* T5 family
  * T5 series 
  * ByT5 
  * ExT5 
* LaMDA 
* FLAN 
* GShard 
* Meena
* PaLM 
* UL2 
* Minerva 


 
## Huawei 

* PanGu (ç›˜å¤) family 
  * PanGu-Alpha 
  * PanGu-Coder
* NEZHA (å“ªå’) 


 
## Inspur 

* Yuan 1.0 

## IDEA

* [å°ç¥žæ¦œâ€”â€”ä¸­æ–‡è¯­è¨€é¢„è®­ç»ƒæ¨¡åž‹å¼€æºè®¡åˆ’](https://fengshenbang-doc.readthedocs.io/zh/latest/index.html)
  * Erlangshen (äºŒéƒŽç¥ž)
  * Wenzhong (é—»ä»²)
  * Rangdeng (ç‡ƒç¯)
  * Yuyuan (ä½™å…ƒ)
  * Bigan (æ¯”å¹²)
  * Zhouwenwang (å‘¨æ–‡çŽ‹)
  * Taiyi (å¤ªä¹™)

> IDEA refers to [ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºæ•°å­—ç»æµŽç ”ç©¶é™¢](https://idea.edu.cn/)

 
## JD (JINGDONG)

* ç»‡å¥³ Vega v1


 
## Langboat 

* Mengzi
  * paper: [Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese](https://arxiv.org/abs/2110.06696)
  * code: https://github.com/Langboat/Mengzi


 
## Microsoft 

* Megatron-Turning NLG family 
* DeBERTa series 
* Turning NLG 
* DeepNet 
* GODEL 
* Metro-LM 
* Z-code M3 


 
## Nvidia

* Megatron-LM 


 
## OpenAI

* GPT-family
  * GPT, GPT-2, GPT-3  
  * WebGPT
  * InstructGPT
* Codex 

 
## OPPO


* OBERT 






 
## Tencent 


* Motian 
* ShenZhou 
* ShenNonG
* TI-NLP
* HunYuan_nlp  


 
## THU

* THUDM
  * Wenhui (æ–‡æ±‡)
    * code: https://github.com/THUDM/Chinese-Transformer-XL

> THUDM refers to "Data Mining Research Group at Tsinghua University".
 
* Tsinghua AI
  * CPM family
    * CPM-1
      * paper: [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413)
      * code: https://github.com/TsinghuaAI/CPM-1-Generate
    * CPM-2
      * paper: [CPM-2: Large-scale cost-effective pre-trained language models](https://www.sciencedirect.com/science/article/pii/S2666651021000310)
      * code: https://github.com/TsinghuaAI/CPM-2-Pretrain

## More PLMs are  coming ...

ðŸ”¥Coming soon!ðŸ”¥


--- 

# Claims 

Thanks for all these companies and orgnizations paying a lot of money and efforts to build and train these large models for the benefit of all human beings. 


> This repo is inspired by [awesome pretrained Chinese NLP models](https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models).


